{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNNeZ7zrrsUp"
      },
      "source": [
        "# TextBlob Coding Help!\n",
        "\n",
        "## Import the required data bases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qss9MVzlCzHX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlalchemy as db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCtY4TCxltTz"
      },
      "source": [
        "## Getting the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wqM-BjGiV9O"
      },
      "outputs": [],
      "source": [
        "pmfeedback = pd.read_csv(\"https://github.com/casbdai/notebooks/raw/main/Module2/HypeCase/pmfeedback.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JdM2J9-Cx0-"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/casbdai/notebooks/raw/main/Module2/HypeCase/hypedb\n",
        "engine = db.create_engine(\"sqlite:///hypedb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFU6R5ZLjs9V"
      },
      "outputs": [],
      "source": [
        "inspector = db.inspect(engine)\n",
        "inspector.get_table_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN5o9Uunl3P3"
      },
      "source": [
        "## Applying LLM-Scoring\n",
        "\n",
        "Run the following code to use a LLM for scoring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_OMR--8Cchw"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from time import sleep\n",
        "\n",
        "client = OpenAI(api_key=\"ENTER YOUR API KEY HERE\")\n",
        "prompt = \"Score the likelihood of a positive sentiment.\"\n",
        "prompt_alternative = \"Score the likelihood of a high originality.\" # Feel free to play around and try out different prompts\n",
        "\n",
        "def score_text(text):\n",
        "  try:\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt + \" Score the text with a likelihood between 0 (very low) and 1 (very high).\\nReturn the score as JSON with the key 'score'. Make sure that only the JSON is returned.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": text[:400]\n",
        "        }\n",
        "    ]\n",
        "    sleep(1)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0\n",
        "    )\n",
        "    return json.loads(response.choices[0].message.content)[\"score\"]\n",
        "  except Exception as e:\n",
        "      print(\"An unexpected error occurred:\", e)\n",
        "      return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jYr2lwImVHl"
      },
      "source": [
        "To apply the code follow the following syntax:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jp9n-iZ_lJj"
      },
      "outputs": [],
      "source": [
        "# dataframe[\"new_column_with_sentiment\"] = dataframe[\"column_to_analyze\"].apply(score_text)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQy4_0mx_lJm"
      },
      "source": [
        "As this code might take some time, you might want to track the progress. For this, use the tqdm library. It adds a .progress_apply function that helps you tracking how much time is needed to execute a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s1vgOmV_lJn"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# dataframe[\"new_column_with_sentiment\"] = dataframe[\"column_to_analyze\"].progress_apply(score_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}